{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "import numpy\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading processed data\n",
    "data = open('cleandata.csv').read()[:100000]\n",
    "\n",
    "#function for preparing text data into sequences for training \n",
    "def data_sequencing(data):   \n",
    "    # integer encode sequences of words\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([data])\n",
    "    with open('tokenizer.pkl', 'wb') as f: # Save the tokeniser by pickling it\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "    # retrieve vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocabulary Size: %d' % vocab_size)\n",
    "    \n",
    "    # create line-based sequences\n",
    "    sequences = list()\n",
    "    rev_sequences = list()\n",
    "    for line in data.split('.'):\n",
    "        encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "        rev_encoded = encoded[::-1]\n",
    "        for i in range(1, len(encoded)):\n",
    "            sequence = encoded[:i+1]\n",
    "            rev_sequence = rev_encoded[:i+1]\n",
    "            sequences.append(sequence)\n",
    "            rev_sequences.append(rev_sequence)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    \n",
    "    #find max sequence length \n",
    "    max_length = max([len(seq) for seq in sequences])\n",
    "    with open('max_length.pkl', 'wb') as f: # Save max_length by pickling it\n",
    "        pickle.dump(max_length, f)\n",
    "    print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "    # pad sequences and create the forward sequence\n",
    "    sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    sequences = array(sequences)\n",
    "    print(sequences[0])\n",
    "    X, y = sequences[:,:-1],sequences[:,-1]\n",
    "    \n",
    "    print(sequences)\n",
    "    print(\"X : \",X)\n",
    "    print(\"Y: \",y)\n",
    "    #pad sequences and create the reverse sequencing\n",
    "    rev_sequences = pad_sequences(rev_sequences, maxlen=max_length, padding='pre')\n",
    "    # split into input and output elements\n",
    "    rev_sequences = array(rev_sequences)\n",
    "    rev_X, rev_y = rev_sequences[:,:-1],rev_sequences[:,-1]\n",
    "\n",
    "    return X,y,rev_X,rev_y,max_length,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3777\n",
      "Total Sequences: 15197\n",
      "Max Sequence Length: 50\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0 193   3]\n",
      "[[   0    0    0 ...    0  193    3]\n",
      " [   0    0    0 ...  193    3  360]\n",
      " [   0    0    0 ...    3  360   19]\n",
      " ...\n",
      " [   0    0    0 ... 3776   48   26]\n",
      " [   0    0    0 ...   48   26 1026]\n",
      " [   0    0    0 ...   26 1026  607]]\n",
      "X :  [[   0    0    0 ...    0    0  193]\n",
      " [   0    0    0 ...    0  193    3]\n",
      " [   0    0    0 ...  193    3  360]\n",
      " ...\n",
      " [   0    0    0 ...    2 3776   48]\n",
      " [   0    0    0 ... 3776   48   26]\n",
      " [   0    0    0 ...   48   26 1026]]\n",
      "Y:  [   3  360   19 ...   26 1026  607]\n"
     ]
    }
   ],
   "source": [
    "#returning forward and reverse sequences along with max sequence \n",
    "#length from the data \n",
    "\n",
    "X,y,rev_X,rev_y,max_length,vocab_size = data_sequencing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define forward sequence model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,100, input_length=max_length-1))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define reverse model\n",
    "rev_model = Sequential()\n",
    "rev_model.add(Embedding(vocab_size, 100, input_length=max_length-1))\n",
    "#rev_model.add(LSTM(100))\n",
    "rev_model.add(Bidirectional(LSTM(100)))\n",
    "rev_model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(rev_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile forward sequence network\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y,batch_size=100, epochs=100, verbose=2)\n",
    "# save the model to file\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile reverse sequence network\n",
    "rev_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "rev_model.fit(rev_X, rev_y,batch_size=100, epochs=100, verbose=2)\n",
    "# save the model to file\n",
    "rev_model.save('rev_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence using a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text):\n",
    "    if seed_text == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        in_text = seed_text\n",
    "        n_words = 1\n",
    "        n_preds = 5 #number of words to predict for the seed text\n",
    "        pred_words = \"\"\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            # pre-pad sequences to a fixed length\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "            # predict probabilities for each word\n",
    "            proba = model.predict(encoded, verbose=0).flatten()\n",
    "            #take the n_preds highest probability classes \n",
    "            yhat = numpy.argsort(-proba)[:n_preds] \n",
    "            # map predicted words index to word\n",
    "            out_word = ''\n",
    "\n",
    "            for _ in range(n_preds):\n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == yhat[_] and word not in stopwords:\n",
    "                        out_word = word\n",
    "                        pred_words += ' ' + out_word\n",
    "                        #print(out_word)\n",
    "                        break\n",
    "\n",
    "\n",
    "        return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize the stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "rev_model = load_model('rev_model.h5')\n",
    "\n",
    "#load tokeniser and max_length\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    \n",
    "with open('max_length.pkl', 'rb') as f:\n",
    "    max_length = pickle.load(f)\n",
    "    \n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and set embeddings for OOV words\n",
    "def set_embedding_for_oov(doc):\n",
    "    #checking for oov words and adding embedding\n",
    "    for token in doc:\n",
    "        if token.is_oov == True:\n",
    "            before_text = doc[:token.i].text\n",
    "            after_text = str(array(doc)[:token.i:-1]).replace('[','').replace(']','')\n",
    "            print(\"Before Text: \",before_text)\n",
    "            print(\"After text: \",after_text )\n",
    "\n",
    "            pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
    "            pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
    "            \n",
    "            embedding = numpy.zeros((300,))\n",
    "\n",
    "            i=len(before_text)\n",
    "            print('Words predicted from forward sequence model:')\n",
    "            for word in pred_before:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            i=len(after_text)\n",
    "            print('Words predicted from reverse sequence model:')\n",
    "            for word in pred_after:\n",
    "                print(word)\n",
    "                embedding += i*nlp.vocab.get_vector(word)\n",
    "                i= i*.5\n",
    "            nlp.vocab.set_vector(token.text, embedding)\n",
    "            print(token.text,nlp.vocab.get_vector(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Text:  i\n",
      "After text:  and london in\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(1, 100), b.shape=(100, 100), m=1, n=100, k=100\n\t [[Node: bidirectional_1/while_1/MatMul_6 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while_1/Switch_3:1, bidirectional_1/while_1/MatMul_6/Enter)]]\n\t [[Node: dense_1/Softmax/_587 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_472_dense_1/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-18e44f904f8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i livwgffe in london and'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mset_embedding_for_oov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-87018b602a90>\u001b[0m in \u001b[0;36mset_embedding_for_oov\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After text: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mafter_text\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mpred_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbefore_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mpred_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-626b6a6d1642>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[1;34m(model, tokenizer, max_length, seed_text)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m# predict probabilities for each word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m#take the n_preds highest probability classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mproba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_preds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfk\\lib\\site-packages\\keras-2.2.4-py3.6.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m   1185\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfk\\lib\\site-packages\\keras-2.2.4-py3.6.egg\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfk\\lib\\site-packages\\keras-2.2.4-py3.6.egg\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfk\\lib\\site-packages\\keras-2.2.4-py3.6.egg\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfk\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfk\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(1, 100), b.shape=(100, 100), m=1, n=100, k=100\n\t [[Node: bidirectional_1/while_1/MatMul_6 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](bidirectional_1/while_1/Switch_3:1, bidirectional_1/while_1/MatMul_6/Enter)]]\n\t [[Node: dense_1/Softmax/_587 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_472_dense_1/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "doc = nlp('i livwgffe in london and')\n",
    "set_embedding_for_oov(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(['and'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[[ 0.03229091,  0.14570677,  0.10186311, ..., -0.3778659 ,\n",
      "          0.10639041,  0.30900633],\n",
      "        [ 0.03229091,  0.14570677,  0.10186311, ..., -0.3778659 ,\n",
      "          0.10639041,  0.30900633],\n",
      "        [ 0.03229091,  0.14570677,  0.10186311, ..., -0.3778659 ,\n",
      "          0.10639041,  0.30900633],\n",
      "        ...,\n",
      "        [ 0.03229091,  0.14570677,  0.10186311, ..., -0.3778659 ,\n",
      "          0.10639041,  0.30900633],\n",
      "        [ 0.03229091,  0.14570677,  0.10186311, ..., -0.3778659 ,\n",
      "          0.10639041,  0.30900633],\n",
      "        [-0.1914773 ,  0.13585521,  0.13554917, ...,  0.06510769,\n",
      "         -0.1267743 ,  0.11291379]]], dtype=float32)], [array([[-0.0000000e+00, -9.8321718e-01, -1.0000000e+00, -7.6140159e-01,\n",
      "        -9.2983049e-01,  5.4122710e-01, -8.7296522e-01, -9.8710352e-01,\n",
      "        -4.4785690e-01, -0.0000000e+00,  0.0000000e+00,  9.9977386e-01,\n",
      "        -9.9959952e-01, -9.9994224e-01, -0.0000000e+00,  0.0000000e+00,\n",
      "        -0.0000000e+00, -0.0000000e+00, -0.0000000e+00, -7.5286365e-01,\n",
      "         7.5671709e-01, -0.0000000e+00,  0.0000000e+00,  2.7736187e-02,\n",
      "         9.8780614e-01,  7.4362016e-01,  5.8220822e-01, -6.4668804e-01,\n",
      "         0.0000000e+00,  1.0000000e+00, -0.0000000e+00,  7.9230708e-01,\n",
      "        -9.9801201e-01, -1.0000000e+00, -9.9979013e-01, -0.0000000e+00,\n",
      "        -0.0000000e+00, -0.0000000e+00, -9.9999619e-01, -0.0000000e+00,\n",
      "         0.0000000e+00,  9.9999398e-01,  5.5285931e-01,  1.0000000e+00,\n",
      "        -2.0431995e-02, -1.0000000e+00,  4.5583856e-01, -0.0000000e+00,\n",
      "        -6.0545498e-01,  9.9599326e-01,  0.0000000e+00,  0.0000000e+00,\n",
      "        -0.0000000e+00, -0.0000000e+00, -9.0776277e-01, -8.8755417e-01,\n",
      "         0.0000000e+00, -0.0000000e+00,  1.0000000e+00, -3.5300112e-01,\n",
      "        -2.0384207e-01, -2.1432571e-02, -7.5362146e-01,  0.0000000e+00,\n",
      "        -5.7981908e-01, -0.0000000e+00, -7.6047754e-01, -1.0000000e+00,\n",
      "         8.6886340e-01, -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "        -9.9988633e-01, -1.0000000e+00,  0.0000000e+00,  9.9997753e-01,\n",
      "         3.9909384e-01, -0.0000000e+00,  2.9338971e-01, -6.5447706e-01,\n",
      "        -9.9994433e-01,  1.1783540e-01, -0.0000000e+00,  0.0000000e+00,\n",
      "        -5.0431687e-01, -0.0000000e+00, -1.0000000e+00, -9.9837220e-01,\n",
      "        -9.9895686e-01, -0.0000000e+00, -7.5524533e-01,  0.0000000e+00,\n",
      "         1.0000000e+00,  0.0000000e+00,  0.0000000e+00, -0.0000000e+00,\n",
      "        -0.0000000e+00,  1.5447715e-01,  9.9959409e-01,  1.0000000e+00,\n",
      "        -7.7616923e-02,  3.6212933e-01,  9.9968034e-01, -5.1477385e-01,\n",
      "         0.0000000e+00,  1.3185665e-07,  0.0000000e+00, -4.9030388e-20,\n",
      "        -0.0000000e+00,  9.9734098e-01,  5.7453811e-01, -0.0000000e+00,\n",
      "         5.9247401e-29, -8.6205524e-01,  4.0048200e-01,  0.0000000e+00,\n",
      "         3.1950951e-01, -1.0196882e-07, -0.0000000e+00, -0.0000000e+00,\n",
      "        -0.0000000e+00, -0.0000000e+00,  1.9396924e-09,  9.4226144e-02,\n",
      "         0.0000000e+00,  5.2925617e-01,  8.9050770e-02,  0.0000000e+00,\n",
      "         2.4391443e-01,  0.0000000e+00, -3.4277439e-01,  2.6130939e-01,\n",
      "         2.3230404e-04,  0.0000000e+00,  0.0000000e+00,  2.2345692e-02,\n",
      "         0.0000000e+00, -1.2736455e-01, -5.7117516e-01,  0.0000000e+00,\n",
      "        -0.0000000e+00,  1.0000000e+00, -0.0000000e+00,  5.6134513e-19,\n",
      "         5.6468523e-01,  1.0000000e+00,  4.9266210e-01, -0.0000000e+00,\n",
      "        -3.7406844e-01, -0.0000000e+00,  1.0000000e+00,  1.0000000e+00,\n",
      "         3.4284213e-01,  1.2172417e-01,  0.0000000e+00,  6.2908947e-01,\n",
      "         3.5195938e-01, -0.0000000e+00, -3.7028331e-01, -1.9908321e-01,\n",
      "        -0.0000000e+00, -2.7623917e-22, -0.0000000e+00,  5.7715261e-01,\n",
      "         0.0000000e+00, -0.0000000e+00,  3.1520647e-01,  0.0000000e+00,\n",
      "        -0.0000000e+00, -0.0000000e+00,  0.0000000e+00, -9.9793702e-01,\n",
      "         0.0000000e+00, -2.9651421e-01,  4.2110167e-02,  0.0000000e+00,\n",
      "        -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  2.3426950e-01,\n",
      "        -2.7061266e-01,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "        -0.0000000e+00,  7.3421530e-02, -2.4609511e-07, -5.6162433e-21,\n",
      "         1.4562997e-01,  1.5335988e-09, -2.2120687e-01,  0.0000000e+00,\n",
      "         0.0000000e+00, -0.0000000e+00,  2.5819546e-01, -8.0953908e-01,\n",
      "        -1.4080049e-01, -3.4445577e-07,  0.0000000e+00, -8.7327585e-02]],\n",
      "      dtype=float32)], [array([[8.4857971e-10, 5.3977840e-07, 2.0801294e-06, ..., 8.9839206e-11,\n",
      "        4.5098281e-09, 2.6642617e-11]], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "inp = model.input                                           # input placeholder\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences(['and'])[0]\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]    # evaluation functions\n",
    "\n",
    "# Testing\n",
    "test =pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "layer_outs = [func([test, 1.]) for func in functors]\n",
    "print (layer_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# with a Sequential model\n",
    "get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[0].output])\n",
    "layer_output = get_3rd_layer_output([test])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.03229091  0.14570677  0.10186311 ... -0.3778659   0.10639041\n",
      "    0.30900633]\n",
      "  [ 0.03229091  0.14570677  0.10186311 ... -0.3778659   0.10639041\n",
      "    0.30900633]\n",
      "  [ 0.03229091  0.14570677  0.10186311 ... -0.3778659   0.10639041\n",
      "    0.30900633]\n",
      "  ...\n",
      "  [ 0.03229091  0.14570677  0.10186311 ... -0.3778659   0.10639041\n",
      "    0.30900633]\n",
      "  [ 0.03229091  0.14570677  0.10186311 ... -0.3778659   0.10639041\n",
      "    0.30900633]\n",
      "  [-0.1914773   0.13585521  0.13554917 ...  0.06510769 -0.1267743\n",
      "    0.11291379]]]\n"
     ]
    }
   ],
   "source": [
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "print(layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.03229091,  0.14570677,  0.10186311, ..., -0.3778659 ,\n",
      "         0.10639041,  0.30900633],\n",
      "       [ 0.4312459 ,  0.8094974 ,  0.5639969 , ..., -0.3482559 ,\n",
      "         0.66176736,  0.34343928],\n",
      "       [ 0.0354765 ,  0.14547107, -0.08885228, ..., -0.04287698,\n",
      "         0.98114103,  1.0777733 ],\n",
      "       ...,\n",
      "       [ 0.29039997,  0.4459584 ,  0.07219367, ...,  0.38600206,\n",
      "        -0.08380733, -0.27997503],\n",
      "       [ 0.34965372,  0.16146661,  0.3918662 , ...,  0.16594979,\n",
      "        -0.10325196, -0.11696597],\n",
      "       [-0.35282457, -0.52569634,  0.10266012, ...,  0.16191858,\n",
      "         0.11232043, -0.19642301]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights() # list of numpy arrays\n",
    "    print(weights)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-89d32c416227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "we = model.layers[0].get_weights()\n",
    "print(we[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfk] *",
   "language": "python",
   "name": "conda-env-tfk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
