{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "import numpy\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence using a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text):\n",
    "    if seed_text == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        in_text = seed_text\n",
    "        n_words = 1\n",
    "        n_preds = 5 #number of words to predict for the seed text\n",
    "        pred_words = \"\"\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            # pre-pad sequences to a fixed length\n",
    "            encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "            # predict probabilities for each word\n",
    "            proba = model.predict(encoded, verbose=0).flatten()\n",
    "            #take the n_preds highest probability classes \n",
    "            yhat = numpy.argsort(-proba)[:n_preds] \n",
    "            # map predicted words index to word\n",
    "            out_word = ''\n",
    "\n",
    "\n",
    "            for _ in range(n_preds):\n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index == yhat[_] and word not in stoplist:\n",
    "                        out_word = word\n",
    "                        pred_words += ' ' + out_word\n",
    "                        #print(out_word)\n",
    "                        break\n",
    "            \n",
    "            reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "                        \n",
    "            '''for k in range(n_preds):\n",
    "                out_word = reverse_word_map[yhat[k]]\n",
    "                if out_word not in stopwords:\n",
    "                    pred_words += ' '+ out_word'''\n",
    "\n",
    "\n",
    "        return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize the stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "rev_model = load_model('rev_model.h5')\n",
    "\n",
    "#load tokeniser and max_length\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    \n",
    "with open('max_length.pkl', 'rb') as f:\n",
    "    max_length = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_embedding_for_oov(doc,i):\n",
    "    #checking for oov words and adding embedding\n",
    "    d = doc.split()\n",
    "    c=0\n",
    "    s = \"\"\n",
    "    for w in d:\n",
    "        c+=1\n",
    "        s = s+\" \"+w\n",
    "        if c==i:\n",
    "            break\n",
    "    s.strip()\n",
    "    before_text = s\n",
    "    c=0\n",
    "    s = \"\"\n",
    "    for w in d:\n",
    "        if c>i:\n",
    "            s = s+\" \"+w\n",
    "        c+=1\n",
    "    s.strip()\n",
    "    after_text = s\n",
    "    pred_before = generate_seq(model, tokenizer, max_length-1, before_text).split()\n",
    "    pred_after = generate_seq(rev_model, tokenizer, max_length-1, after_text).split()\n",
    "            \n",
    "    embedding = numpy.zeros((1,100))\n",
    "    \n",
    "    we = model.layers[0].get_weights()[0]\n",
    "    \n",
    "    i=len(before_text)\n",
    "    print('Words predicted from forward sequence model:')\n",
    "    for word in pred_before:\n",
    "        print(word)\n",
    "        \n",
    "        embedding += i*(we[tokenizer.texts_to_sequences([word])[0]])\n",
    "        i= i*.5\n",
    "    i=len(after_text)\n",
    "    print('Words predicted from reverse sequence model:')\n",
    "    for word in pred_after:\n",
    "        print(word)\n",
    "        embedding += i*(we[tokenizer.texts_to_sequences([word])[0]])\n",
    "        i= i*.5\n",
    "    #print(\"livwgffe\")\n",
    "    #print(we)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cosine_sim(x,y):\n",
    "    return cosine_similarity(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_score(wvec):\n",
    "    we = model.layers[0].get_weights()[0]\n",
    "    a = cosine_sim(we,wvec)\n",
    "    d={}\n",
    "    \n",
    "    for w,i in tokenizer.word_index.items():\n",
    "        d[w] = list(a[i])[0]\n",
    "    bag = d\n",
    "    bag = sorted(bag.items(), key=lambda x:x[1],reverse = True)\n",
    "    i=0\n",
    "    wl = []\n",
    "    for word,sim in bag:\n",
    "        if word not in stoplist:\n",
    "            i+=1\n",
    "            wl.append(word)\n",
    "            if i == 5:\n",
    "                break\n",
    "                \n",
    "    print(\"Similar words: \",wl)\n",
    "    return wl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('ner_dataset.csv', encoding = 'latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(l):\n",
    "    percent = 1.0\n",
    "    word = df[\"Word\"].tolist()\n",
    "    tag = df[\"Tag\"].tolist()\n",
    "    \n",
    "    t = {}\n",
    "    for w in l:\n",
    "        c = 5\n",
    "        try:\n",
    "            if tag[word.index(w)] is not 'O':\n",
    "                c = 15\n",
    "            if tag[word.index(w)] in t.keys():\n",
    "                p = t[tag[word.index(w)]]\n",
    "                t[tag[word.index(w)]] = p + c*percent\n",
    "            else:\n",
    "                t[tag[word.index(w)]] = c*percent\n",
    "            percent = percent - 0.1\n",
    "        except:\n",
    "            w = w.title()\n",
    "            if tag[word.index(w)] is not 'O':\n",
    "                c = 15\n",
    "            if tag[word.index(w)] in t.keys():\n",
    "                p = t[tag[word.index(w)]]\n",
    "                t[tag[word.index(w)]] = p + c*percent\n",
    "            else:\n",
    "                t[tag[word.index(w)]] = c*percent\n",
    "            percent = percent - 0.1\n",
    "    \n",
    "    print(t)\n",
    "    \n",
    "    tag = sorted(t.items(), key=lambda x:x[1],reverse = True)[0]\n",
    "    print(\"Tag is : \",tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag(doc,i):\n",
    "    wvec = set_embedding_for_oov(doc,i)\n",
    "    wl = set_score(wvec)\n",
    "    tag(wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words predicted from forward sequence model:\n",
      "iraq\n",
      "several\n",
      "baghdad\n",
      "northern\n",
      "Words predicted from reverse sequence model:\n",
      "Similar words:  ['iraq', 'different', 'areas', 'schools', 'argentina']\n",
      "{'B-geo': 15.0, 'O': 12.0, 'B-org': 9.000000000000002}\n",
      "Tag is :  B-geo\n"
     ]
    }
   ],
   "source": [
    "doc = 'two days of rescue operations in Arunachal'\n",
    "find_tag(doc,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words predicted from forward sequence model:\n",
      "palestinian\n",
      "war\n",
      "two\n",
      "one\n",
      "afghan\n",
      "Words predicted from reverse sequence model:\n",
      "deaths\n",
      "prisoners\n",
      "living\n",
      "home\n",
      "kidnapped\n",
      "Similar words:  ['palestinian', 'deaths', 'living', 'security', 'service']\n",
      "{'B-gpe': 15.0, 'O': 15.0}\n",
      "Tag is :  B-gpe\n"
     ]
    }
   ],
   "source": [
    "doc = '26 children workers rescued from Parle-G plant in Chhattisgarh'\n",
    "find_tag(doc,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words predicted from forward sequence model:\n",
      "indian\n",
      "march\n",
      "american\n",
      "jerusalem\n",
      "Words predicted from reverse sequence model:\n",
      "biathlon\n",
      "cities\n",
      "bronchitis\n",
      "darfur\n",
      "election\n",
      "Similar words:  ['indian', 'staged', 'biathlon', 'kill', 'foundations']\n",
      "{'B-geo': 15.0, 'O': 15.0}\n",
      "Tag is :  B-geo\n"
     ]
    }
   ],
   "source": [
    "doc = 'Some of the children allegedly employed at Apple factory hailed from Odisha and Jharkhand'\n",
    "find_tag(doc,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfk] *",
   "language": "python",
   "name": "conda-env-tfk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
